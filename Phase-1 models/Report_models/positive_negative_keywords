import os
import json
import torch
from transformers import pipeline
from typing import List
from pathlib import Path

# Directories
base_dir = os.getcwd()
output_dir = os.path.join(base_dir, "Keywords")
os.makedirs(output_dir, exist_ok=True)

# Local model directory
model_dir = os.path.join(base_dir, "bart-large-mnli")

# Device config
device = 0 if torch.cuda.is_available() else -1

# Load zero-shot classifier pipeline from local directory
try:
    zero_shot_classifier = pipeline(
        "zero-shot-classification",
        model=model_dir,
        device=device
    )
except Exception as e:
    print(f"Error loading model from {model_dir}: {e}")
    print("Ensure the 'bart-large-mnli' directory contains the model files. Run 'bart_mnli_download.py' if missing.")
    exit(1)

# Candidate labels for keyword extraction
candidate_labels = ["Positive Keyword", "Negative Keyword"]

# Stop words: prepositions, pronouns, and non-indicative words
stop_words = {
    'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'for', 'from', 'of',
    'with', 'to', 'by', 'about', 'is', 'are', 'was', 'were', 'this', 'that', 'these',
    'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us',
    'them', 'your', 'please', 'okay', 'hi', 'what', 'whats', 'if', 'can', 'have'
}

def classify_keyword(word: str) -> tuple:
    """Classify a single word as a positive or negative keyword."""
    try:
        result = zero_shot_classifier(word, candidate_labels, multi_label=False)
        print(f"Raw pipeline result for word '{word}': {result}")
        top_label = result['labels'][0]
        score = result['scores'][0]
        scores_dict = dict(zip(result['labels'], result['scores']))
        return top_label, score, scores_dict
    except Exception as e:
        print(f"Error classifying word '{word}': {e}")
        return None, None, None

def analyze_promises(json_file: str) -> List[dict]:
    """Analyze a single JSON file and extract positive and negative keywords."""
    json_path = Path(json_file)
    if not json_path.is_file():
        print(f"File not found: {json_file}")
        return None

    with open(json_path, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
        except Exception as e:
            print(f"Failed to load JSON from {json_file}: {e}")
            return None

    transcription = data if isinstance(data, list) else data.get("serializedTranscription", [])
    if not isinstance(transcription, list) or not all("dialogue" in d for d in transcription):
        print(f"Malformed transcription in {json_file}")
        return None

    full_conversation = " ".join(d["dialogue"] for d in transcription)

    # Break transcription into single words and filter out stop words
    words = [word.lower() for word in full_conversation.split() if word.lower() not in stop_words]

    # Classify each word and collect keywords
    positive_keywords = []
    negative_keywords = []
    threshold = 0.85  # Confidence threshold for strict selection

    for word in words:
        top_label, score, scores_dict = classify_keyword(word)
        if top_label is None:
            continue  # Skip if classification failed
        if top_label == "Positive Keyword" and score >= threshold:
            positive_keywords.append((word, score))
        elif top_label == "Negative Keyword" and score >= threshold:
            negative_keywords.append((word, score))

    # Remove duplicates and sort by score
    positive_keywords = list(set(positive_keywords))  # Convert to set to remove duplicates
    negative_keywords = list(set(negative_keywords))
    positive_keywords.sort(key=lambda x: x[1], reverse=True)  # Sort by score
    negative_keywords.sort(key=lambda x: x[1], reverse=True)

    # Extract just the words for console output
    positive_keyword_list = [kw[0] for kw in positive_keywords]
    negative_keyword_list = [kw[0] for kw in negative_keywords]

    # Save output
    base_filename = os.path.splitext(json_path.name)[0]
    output_file = os.path.join(output_dir, f"{base_filename}_keywords.txt")
    with open(output_file, "w", encoding="utf-8") as f_output:
        f_output.write("Positive Keywords:\n")
        f_output.write("\n".join(f"{kw} (score: {score:.3f})" for kw, score in positive_keywords) + "\n\n")
        f_output.write("Negative Keywords:\n")
        f_output.write("\n".join(f"{kw} (score: {score:.3f})" for kw, score in negative_keywords))

    print(f"[✔] {base_filename} → Positive Keywords: {positive_keyword_list}")
    print(f"[✔] {base_filename} → Negative Keywords: {negative_keyword_list}")
    return transcription

def process_all_json_files(directory: str):
    """Process all JSON files in the specified directory."""
    json_files = [f for f in os.listdir(directory) if f.endswith(".json")]
    if not json_files:
        print(f"No JSON files found in {directory}")
        return

    for json_file in json_files:
        analyze_promises(os.path.join(directory, json_file))

if __name__ == "__main__":
    # Process a single file
    json_file = "transcription.json"
    analyze_promises(json_file)

    # Optionally process all JSON files in the directory
    # print(f"Processing JSON files in {base_dir}")
    # process_all_json_files(base_dir)